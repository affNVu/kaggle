{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Lin Reg Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, linear_model, model_selection, pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10_000, 93)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>feature_31</th><th>feature_32</th><th>&hellip;</th><th>feature_52</th><th>feature_53</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th><th>responder_0</th><th>responder_1</th><th>responder_2</th><th>responder_3</th><th>responder_4</th><th>responder_5</th><th>responder_6</th><th>responder_7</th><th>responder_8</th><th>partition_id</th></tr><tr><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i8</td><td>i8</td><td>i16</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td><td>3.889038</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.851033</td><td>0.242971</td><td>0.2634</td><td>-0.891687</td><td>11</td><td>7</td><td>76</td><td>-0.883028</td><td>0.003067</td><td>-0.744703</td><td>null</td><td>-0.169586</td><td>null</td><td>-1.335938</td><td>-1.707803</td><td>0.91013</td><td>null</td><td>1.636431</td><td>1.522133</td><td>-1.551398</td><td>-0.229627</td><td>null</td><td>null</td><td>1.378301</td><td>-0.283712</td><td>0.123196</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>-0.808103</td><td>null</td><td>-2.037683</td><td>0.727661</td><td>null</td><td>-0.989118</td><td>-0.345213</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-1.251104</td><td>-0.110252</td><td>-0.491157</td><td>-1.02269</td><td>0.152241</td><td>-0.659864</td><td>null</td><td>null</td><td>-0.261412</td><td>-0.211486</td><td>-0.335556</td><td>-0.281498</td><td>0.738489</td><td>-0.069556</td><td>1.380875</td><td>2.005353</td><td>0.186018</td><td>1.218368</td><td>0.775981</td><td>0.346999</td><td>0.095504</td><td>0</td></tr><tr><td>0</td><td>0</td><td>7</td><td>1.370613</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.676961</td><td>0.151984</td><td>0.192465</td><td>-0.521729</td><td>11</td><td>7</td><td>76</td><td>-0.865307</td><td>-0.225629</td><td>-0.582163</td><td>null</td><td>0.317467</td><td>null</td><td>-1.250016</td><td>-1.682929</td><td>1.412757</td><td>null</td><td>0.520378</td><td>0.744132</td><td>-0.788658</td><td>0.641776</td><td>null</td><td>null</td><td>0.2272</td><td>0.580907</td><td>1.128879</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>-1.625862</td><td>null</td><td>-1.410017</td><td>1.063013</td><td>null</td><td>0.888355</td><td>0.467994</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-1.065759</td><td>0.013322</td><td>-0.592855</td><td>-1.052685</td><td>-0.393726</td><td>-0.741603</td><td>null</td><td>null</td><td>-0.281207</td><td>-0.182894</td><td>-0.245565</td><td>-0.302441</td><td>2.965889</td><td>1.190077</td><td>-0.523998</td><td>3.849921</td><td>2.626981</td><td>5.0</td><td>0.703665</td><td>0.216683</td><td>0.778639</td><td>0</td></tr><tr><td>0</td><td>0</td><td>9</td><td>2.285698</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.056285</td><td>0.187227</td><td>0.249901</td><td>-0.77305</td><td>11</td><td>7</td><td>76</td><td>-0.675719</td><td>-0.199404</td><td>-0.586798</td><td>null</td><td>-0.814909</td><td>null</td><td>-1.296782</td><td>-2.040234</td><td>0.639589</td><td>null</td><td>1.597359</td><td>0.657514</td><td>-1.350148</td><td>0.364215</td><td>null</td><td>null</td><td>-0.017751</td><td>-0.317361</td><td>-0.122379</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>-0.72542</td><td>null</td><td>-2.29417</td><td>1.764551</td><td>null</td><td>-0.120789</td><td>-0.063458</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.882604</td><td>-0.072482</td><td>-0.617934</td><td>-0.86323</td><td>-0.241892</td><td>-0.709919</td><td>null</td><td>null</td><td>0.377131</td><td>0.300724</td><td>-0.106842</td><td>-0.096792</td><td>-0.864488</td><td>-0.280303</td><td>-0.326697</td><td>0.375781</td><td>1.271291</td><td>0.099793</td><td>2.109352</td><td>0.670881</td><td>0.772828</td><td>0</td></tr><tr><td>0</td><td>0</td><td>10</td><td>0.690606</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.139366</td><td>0.273328</td><td>0.306549</td><td>-1.262223</td><td>42</td><td>5</td><td>150</td><td>-0.694008</td><td>3.004091</td><td>0.114809</td><td>null</td><td>-0.251882</td><td>null</td><td>-1.902009</td><td>-0.979447</td><td>0.241165</td><td>null</td><td>-0.392359</td><td>-0.224699</td><td>-2.129397</td><td>-0.855287</td><td>null</td><td>null</td><td>0.404142</td><td>-0.578156</td><td>0.105702</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>1.313203</td><td>null</td><td>-0.810125</td><td>2.939022</td><td>null</td><td>3.988801</td><td>1.834661</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.697595</td><td>1.074309</td><td>-0.206929</td><td>-0.530602</td><td>4.765215</td><td>0.571554</td><td>null</td><td>null</td><td>-0.226891</td><td>-0.251412</td><td>-0.215522</td><td>-0.296244</td><td>0.408499</td><td>0.223992</td><td>2.294888</td><td>1.097444</td><td>1.225872</td><td>1.225376</td><td>1.114137</td><td>0.775199</td><td>-1.379516</td><td>0</td></tr><tr><td>0</td><td>0</td><td>14</td><td>0.44057</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.9552</td><td>0.262404</td><td>0.344457</td><td>-0.613813</td><td>44</td><td>3</td><td>16</td><td>-0.947351</td><td>-0.030018</td><td>-0.502379</td><td>null</td><td>0.646086</td><td>null</td><td>-1.844685</td><td>-1.58656</td><td>-0.182024</td><td>null</td><td>-0.969949</td><td>-0.673813</td><td>-1.282132</td><td>-1.399894</td><td>null</td><td>null</td><td>0.043815</td><td>-0.320225</td><td>-0.031713</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>0.476195</td><td>null</td><td>-0.771732</td><td>2.843421</td><td>null</td><td>1.379815</td><td>0.411827</td><td>-1.36224</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.948601</td><td>-0.136814</td><td>-0.447704</td><td>-1.141761</td><td>0.099631</td><td>-0.661928</td><td>null</td><td>null</td><td>3.678076</td><td>2.793581</td><td>2.61825</td><td>3.418133</td><td>-0.373387</td><td>-0.502764</td><td>-0.348021</td><td>-3.928148</td><td>-1.591366</td><td>-5.0</td><td>-3.57282</td><td>-1.089123</td><td>-5.0</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1</td><td>266</td><td>38</td><td>0.957651</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.080293</td><td>0.005216</td><td>0.510975</td><td>-0.365682</td><td>50</td><td>1</td><td>522</td><td>1.122625</td><td>-0.546552</td><td>-0.030923</td><td>0.272676</td><td>0.495328</td><td>0.227373</td><td>-2.478088</td><td>-0.044577</td><td>-0.089368</td><td>null</td><td>-0.292053</td><td>0.144377</td><td>-0.552664</td><td>1.26902</td><td>null</td><td>null</td><td>0.310711</td><td>0.120922</td><td>0.15192</td><td>null</td><td>-0.456278</td><td>&hellip;</td><td>-0.261433</td><td>0.703524</td><td>1.325452</td><td>-0.113482</td><td>0.8732</td><td>-0.223273</td><td>0.105094</td><td>-0.006827</td><td>0.182261</td><td>-1.081097</td><td>-0.280307</td><td>-0.317654</td><td>-0.369233</td><td>-0.363336</td><td>0.250411</td><td>0.462447</td><td>-0.432984</td><td>-0.508954</td><td>1.850114</td><td>-0.478685</td><td>0.465432</td><td>-0.218159</td><td>-0.197322</td><td>-0.255259</td><td>-0.226196</td><td>-0.337135</td><td>-0.124516</td><td>0.059029</td><td>3.06932</td><td>0.001147</td><td>-0.29603</td><td>2.27644</td><td>-0.099191</td><td>-0.524028</td><td>-0.913895</td><td>-0.336975</td><td>0</td></tr><tr><td>1</td><td>267</td><td>0</td><td>1.749479</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.124629</td><td>-0.083332</td><td>0.483137</td><td>-0.594018</td><td>11</td><td>7</td><td>76</td><td>0.904611</td><td>0.339676</td><td>0.084029</td><td>0.116143</td><td>0.54328</td><td>0.116928</td><td>-0.016124</td><td>-0.291417</td><td>1.039095</td><td>null</td><td>0.756488</td><td>0.432157</td><td>-1.626238</td><td>-0.137137</td><td>null</td><td>null</td><td>0.952274</td><td>-0.186008</td><td>0.127051</td><td>null</td><td>0.502251</td><td>&hellip;</td><td>0.341569</td><td>-0.337329</td><td>-1.295527</td><td>-0.788592</td><td>-0.037559</td><td>-0.517898</td><td>-0.262303</td><td>-1.815244</td><td>-0.215217</td><td>-1.081097</td><td>-0.251122</td><td>-0.386433</td><td>-0.371299</td><td>0.289167</td><td>-0.143315</td><td>1.302781</td><td>0.633717</td><td>0.030439</td><td>1.224101</td><td>-0.302023</td><td>0.145384</td><td>-0.376314</td><td>-0.240181</td><td>-0.197539</td><td>-0.257807</td><td>-0.291366</td><td>-0.258</td><td>-0.004977</td><td>-0.452896</td><td>-0.263653</td><td>0.130251</td><td>-1.894721</td><td>-0.305431</td><td>0.229617</td><td>0.191162</td><td>-0.156025</td><td>0</td></tr><tr><td>1</td><td>267</td><td>1</td><td>3.911768</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.088497</td><td>-0.117414</td><td>0.333338</td><td>-0.478604</td><td>11</td><td>7</td><td>76</td><td>0.430371</td><td>-0.337503</td><td>-0.307166</td><td>0.053169</td><td>-0.434035</td><td>-0.163834</td><td>1.155249</td><td>-0.334988</td><td>0.751294</td><td>null</td><td>2.282396</td><td>1.263641</td><td>-1.622081</td><td>-0.294155</td><td>null</td><td>null</td><td>1.758774</td><td>-0.304175</td><td>0.141651</td><td>null</td><td>0.105529</td><td>&hellip;</td><td>-0.560273</td><td>0.888622</td><td>-0.933111</td><td>-0.318709</td><td>-0.633239</td><td>-0.308986</td><td>-0.1461</td><td>-0.352973</td><td>0.17185</td><td>-1.081097</td><td>-0.070526</td><td>-0.494284</td><td>-0.417775</td><td>-0.827501</td><td>0.179873</td><td>0.309284</td><td>-0.271827</td><td>-0.559964</td><td>0.50919</td><td>-0.291523</td><td>-0.230026</td><td>-0.309465</td><td>-0.452241</td><td>-0.308508</td><td>-0.249417</td><td>-0.268153</td><td>-0.32052</td><td>0.514781</td><td>0.335962</td><td>-0.052641</td><td>0.210978</td><td>0.187848</td><td>-0.314414</td><td>-0.216178</td><td>0.416633</td><td>-0.387448</td><td>0</td></tr><tr><td>1</td><td>267</td><td>2</td><td>1.062098</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0874</td><td>-0.075169</td><td>0.520792</td><td>-0.331104</td><td>81</td><td>2</td><td>59</td><td>-0.051712</td><td>-0.586925</td><td>-0.541779</td><td>-0.357879</td><td>0.008326</td><td>-0.908561</td><td>-0.334618</td><td>-0.055351</td><td>-0.830805</td><td>null</td><td>-0.238425</td><td>0.028479</td><td>-1.182548</td><td>0.392532</td><td>null</td><td>null</td><td>0.341612</td><td>-0.521844</td><td>-0.178283</td><td>null</td><td>0.011028</td><td>&hellip;</td><td>-2.006263</td><td>0.7918</td><td>-0.406426</td><td>0.312431</td><td>0.515853</td><td>0.499015</td><td>-0.093876</td><td>0.093261</td><td>-0.053561</td><td>-1.081097</td><td>-0.367747</td><td>-0.366597</td><td>-0.586798</td><td>null</td><td>null</td><td>-0.180915</td><td>-0.408936</td><td>-0.565962</td><td>0.064536</td><td>-0.404072</td><td>-0.506974</td><td>0.031831</td><td>0.069683</td><td>-0.314081</td><td>-0.203121</td><td>0.065255</td><td>0.131462</td><td>0.503267</td><td>-0.539222</td><td>0.018141</td><td>1.447976</td><td>-0.40084</td><td>-0.196011</td><td>0.567507</td><td>-0.025589</td><td>-0.281571</td><td>0</td></tr><tr><td>1</td><td>267</td><td>7</td><td>1.083765</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.073229</td><td>-0.073424</td><td>0.454612</td><td>-0.572331</td><td>11</td><td>7</td><td>76</td><td>-0.038701</td><td>-0.104801</td><td>0.072242</td><td>0.550041</td><td>-0.714346</td><td>0.081773</td><td>-0.485693</td><td>0.047385</td><td>2.111038</td><td>null</td><td>0.484647</td><td>1.033527</td><td>-0.771244</td><td>0.504023</td><td>null</td><td>null</td><td>0.240051</td><td>0.634331</td><td>1.433624</td><td>null</td><td>0.220845</td><td>&hellip;</td><td>-1.007623</td><td>-0.671289</td><td>0.693519</td><td>-1.015406</td><td>0.057859</td><td>0.116268</td><td>-0.017977</td><td>-0.14933</td><td>0.561211</td><td>-1.081097</td><td>0.077617</td><td>-0.350327</td><td>-0.289847</td><td>-0.338936</td><td>-0.147619</td><td>-0.202984</td><td>0.097611</td><td>-0.315978</td><td>0.224102</td><td>-0.376514</td><td>0.754986</td><td>-0.372362</td><td>-0.35241</td><td>-0.35863</td><td>-0.333413</td><td>-0.244773</td><td>-0.398434</td><td>-0.428837</td><td>-0.234297</td><td>-0.595952</td><td>-0.908775</td><td>-0.575761</td><td>-0.551953</td><td>0.034488</td><td>0.275154</td><td>0.496757</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10_000, 93)\n",
       "┌─────────┬─────────┬───────────┬──────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ date_id ┆ time_id ┆ symbol_id ┆ weight   ┆ … ┆ responder_ ┆ responder_ ┆ responder_ ┆ partition_ │\n",
       "│ ---     ┆ ---     ┆ ---       ┆ ---      ┆   ┆ 6          ┆ 7          ┆ 8          ┆ id         │\n",
       "│ i16     ┆ i16     ┆ i8        ┆ f32      ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│         ┆         ┆           ┆          ┆   ┆ f32        ┆ f32        ┆ f32        ┆ i64        │\n",
       "╞═════════╪═════════╪═══════════╪══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 0       ┆ 0       ┆ 1         ┆ 3.889038 ┆ … ┆ 0.775981   ┆ 0.346999   ┆ 0.095504   ┆ 0          │\n",
       "│ 0       ┆ 0       ┆ 7         ┆ 1.370613 ┆ … ┆ 0.703665   ┆ 0.216683   ┆ 0.778639   ┆ 0          │\n",
       "│ 0       ┆ 0       ┆ 9         ┆ 2.285698 ┆ … ┆ 2.109352   ┆ 0.670881   ┆ 0.772828   ┆ 0          │\n",
       "│ 0       ┆ 0       ┆ 10        ┆ 0.690606 ┆ … ┆ 1.114137   ┆ 0.775199   ┆ -1.379516  ┆ 0          │\n",
       "│ 0       ┆ 0       ┆ 14        ┆ 0.44057  ┆ … ┆ -3.57282   ┆ -1.089123  ┆ -5.0       ┆ 0          │\n",
       "│ …       ┆ …       ┆ …         ┆ …        ┆ … ┆ …          ┆ …          ┆ …          ┆ …          │\n",
       "│ 1       ┆ 266     ┆ 38        ┆ 0.957651 ┆ … ┆ -0.524028  ┆ -0.913895  ┆ -0.336975  ┆ 0          │\n",
       "│ 1       ┆ 267     ┆ 0         ┆ 1.749479 ┆ … ┆ 0.229617   ┆ 0.191162   ┆ -0.156025  ┆ 0          │\n",
       "│ 1       ┆ 267     ┆ 1         ┆ 3.911768 ┆ … ┆ -0.216178  ┆ 0.416633   ┆ -0.387448  ┆ 0          │\n",
       "│ 1       ┆ 267     ┆ 2         ┆ 1.062098 ┆ … ┆ 0.567507   ┆ -0.025589  ┆ -0.281571  ┆ 0          │\n",
       "│ 1       ┆ 267     ┆ 7         ┆ 1.083765 ┆ … ┆ 0.034488   ┆ 0.275154   ┆ 0.496757   ┆ 0          │\n",
       "└─────────┴─────────┴───────────┴──────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data handling\n",
    "input_paths = 'inputs/train.parquet/*/*.parquet'\n",
    "scan_df = pl.scan_parquet(input_paths)\n",
    "\n",
    "sample_df = scan_df.head(int(1e4)).collect()\n",
    "sample_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 93) (2000, 93) (8000,) (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Specify columns to drop\n",
    "columns = sample_df.columns\n",
    "target_cols = [x for x in sample_df.columns if 'responder' in x]\n",
    "columns_to_drop = ['date_id', 'time_id', 'symbol_id', 'partition_id'] + target_cols\n",
    "\n",
    "#\n",
    "train_df = sample_df\n",
    "target = 'responder_6'\n",
    "train_cols = list(set(columns) - set(target))\n",
    "X = train_df[train_cols]\n",
    "y = train_df[target]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/autocio/lib/python3.11/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: [ 3  5 21 25 30 45 50 58 67]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniconda/base/envs/autocio/lib/python3.11/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: [ 3  5 21 25 30 45 50 58 67]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_df = sample_df.to_pandas()\n",
    "\n",
    "preproc = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        ('drop_columns', ColumnTransformer(transformers=[('column_dropper', 'drop', columns_to_drop)], remainder='passthrough')),\n",
    "        ('impute', SimpleImputer(strategy='mean')),\n",
    "        ('min_max', preprocessing.MinMaxScaler()),\n",
    "        ('norm', preprocessing.StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "X_train_trf = preproc.fit_transform(X_train)\n",
    "X_test_trf = preproc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.41272905\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_000, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>test</th><th>pred</th></tr><tr><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>-0.025232</td><td>-0.000961</td></tr><tr><td>-0.078556</td><td>-0.000961</td></tr><tr><td>-0.391851</td><td>-0.000961</td></tr><tr><td>-0.0157</td><td>-0.000961</td></tr><tr><td>0.434426</td><td>-0.000961</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>-0.70946</td><td>-0.000961</td></tr><tr><td>-0.229555</td><td>-0.000961</td></tr><tr><td>-0.11572</td><td>-0.000961</td></tr><tr><td>0.278669</td><td>-0.000961</td></tr><tr><td>0.223796</td><td>-0.000961</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2_000, 2)\n",
       "┌───────────┬───────────┐\n",
       "│ test      ┆ pred      │\n",
       "│ ---       ┆ ---       │\n",
       "│ f32       ┆ f32       │\n",
       "╞═══════════╪═══════════╡\n",
       "│ -0.025232 ┆ -0.000961 │\n",
       "│ -0.078556 ┆ -0.000961 │\n",
       "│ -0.391851 ┆ -0.000961 │\n",
       "│ -0.0157   ┆ -0.000961 │\n",
       "│ 0.434426  ┆ -0.000961 │\n",
       "│ …         ┆ …         │\n",
       "│ -0.70946  ┆ -0.000961 │\n",
       "│ -0.229555 ┆ -0.000961 │\n",
       "│ -0.11572  ┆ -0.000961 │\n",
       "│ 0.278669  ┆ -0.000961 │\n",
       "│ 0.223796  ┆ -0.000961 │\n",
       "└───────────┴───────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = linear_model.Lasso(alpha=0.1)\n",
    "model.fit(X=X_train_trf, y=y_train)\n",
    "y_pred = model.predict(X_test_trf)\n",
    "print('MSE:', mean_squared_error(y_test, y_pred))\n",
    "pl.DataFrame({'test':y_test, 'pred':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.36161366\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_000, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>test</th><th>pred</th></tr><tr><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>-0.025232</td><td>-0.198202</td></tr><tr><td>-0.078556</td><td>0.133886</td></tr><tr><td>-0.391851</td><td>0.264516</td></tr><tr><td>-0.0157</td><td>-0.16878</td></tr><tr><td>0.434426</td><td>0.12673</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>-0.70946</td><td>0.44443</td></tr><tr><td>-0.229555</td><td>0.089068</td></tr><tr><td>-0.11572</td><td>-0.537979</td></tr><tr><td>0.278669</td><td>-0.685307</td></tr><tr><td>0.223796</td><td>0.098806</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2_000, 2)\n",
       "┌───────────┬───────────┐\n",
       "│ test      ┆ pred      │\n",
       "│ ---       ┆ ---       │\n",
       "│ f32       ┆ f32       │\n",
       "╞═══════════╪═══════════╡\n",
       "│ -0.025232 ┆ -0.198202 │\n",
       "│ -0.078556 ┆ 0.133886  │\n",
       "│ -0.391851 ┆ 0.264516  │\n",
       "│ -0.0157   ┆ -0.16878  │\n",
       "│ 0.434426  ┆ 0.12673   │\n",
       "│ …         ┆ …         │\n",
       "│ -0.70946  ┆ 0.44443   │\n",
       "│ -0.229555 ┆ 0.089068  │\n",
       "│ -0.11572  ┆ -0.537979 │\n",
       "│ 0.278669  ┆ -0.685307 │\n",
       "│ 0.223796  ┆ 0.098806  │\n",
       "└───────────┴───────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = linear_model.Ridge()\n",
    "model.fit(X=X_train_trf, y=y_train)\n",
    "y_pred = model.predict(X_test_trf)\n",
    "print('MSE:', mean_squared_error(y_test, y_pred))\n",
    "pl.DataFrame({'test':y_test, 'pred':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.17014073427999715\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_000, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>test</th><th>pred</th></tr><tr><td>f32</td><td>f64</td></tr></thead><tbody><tr><td>-0.025232</td><td>-0.042458</td></tr><tr><td>-0.078556</td><td>0.048043</td></tr><tr><td>-0.391851</td><td>-0.302542</td></tr><tr><td>-0.0157</td><td>-0.22303</td></tr><tr><td>0.434426</td><td>0.062831</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>-0.70946</td><td>-0.501093</td></tr><tr><td>-0.229555</td><td>-0.17808</td></tr><tr><td>-0.11572</td><td>-0.579353</td></tr><tr><td>0.278669</td><td>-0.646069</td></tr><tr><td>0.223796</td><td>0.162412</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2_000, 2)\n",
       "┌───────────┬───────────┐\n",
       "│ test      ┆ pred      │\n",
       "│ ---       ┆ ---       │\n",
       "│ f32       ┆ f64       │\n",
       "╞═══════════╪═══════════╡\n",
       "│ -0.025232 ┆ -0.042458 │\n",
       "│ -0.078556 ┆ 0.048043  │\n",
       "│ -0.391851 ┆ -0.302542 │\n",
       "│ -0.0157   ┆ -0.22303  │\n",
       "│ 0.434426  ┆ 0.062831  │\n",
       "│ …         ┆ …         │\n",
       "│ -0.70946  ┆ -0.501093 │\n",
       "│ -0.229555 ┆ -0.17808  │\n",
       "│ -0.11572  ┆ -0.579353 │\n",
       "│ 0.278669  ┆ -0.646069 │\n",
       "│ 0.223796  ┆ 0.162412  │\n",
       "└───────────┴───────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVR(C=1.0, epsilon=0.2)\n",
    "model.fit(X=X_train_trf, y=y_train)\n",
    "y_pred = model.predict(X_test_trf)\n",
    "print('MSE:', mean_squared_error(y_test, y_pred))\n",
    "pl.DataFrame({'test':y_test, 'pred':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test path\n",
    "input_paths = 'inputs/test.parquet/*/*.parquet'\n",
    "test_df = pl.scan_parquet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autocio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

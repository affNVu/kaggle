{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def objective(trial, x_train, x_test, y_train, y_test):\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        # \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200, step=50),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    bst = xgb.XGBRegressor(**param)\n",
    "    fmse = make_scorer(mean_squared_error)\n",
    "    mean_cv = np.mean(cross_val_score(bst, x_train, y_train, cv=5, scoring=fmse))\n",
    "    return mean_cv\n",
    "\n",
    "def get_dataset(\n",
    "    cols: list[str],\n",
    "    input_paths: str = 'inputs/train.parquet/*/*.parquet',\n",
    "    fraction: float = 0.1,\n",
    "    head:int =None,\n",
    ") -> pd.DataFrame:\n",
    "    # data handling\n",
    "    lf = pl.scan_parquet(input_paths)\n",
    "    head = lf.select(pl.len()).collect()['len'][0] if head is None else head\n",
    "    df = lf.head(head).select(cols).collect()\n",
    "    df = df.sample(fraction=fraction).to_pandas()\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_single_model(X: pl.DataFrame, y: pl.DataFrame, model_name: str, n_trials: int = 100, timeout: int = 600, cv_frac: float = .1):\n",
    "    _, X_cv, _, y_cv = train_test_split(X, y, test_size=cv_frac)\n",
    "    print('CV Set: ', X_cv.shape)\n",
    "    X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_cv, y_cv, test_size=0.3)\n",
    "    _objective = partial(objective, x_train=X_train_cv, y_train=y_train_cv, x_test=X_test_cv, y_test=y_test_cv)\n",
    "    # hyper tune\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\n",
    "    study.optimize(_objective, n_trials=n_trials, timeout=timeout)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    params = {\n",
    "        \"verbosity\": 2,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        **trial.params,\n",
    "    }\n",
    "    print(params)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05)\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n",
    "    model.save_model(f'{model_name}.json')\n",
    "    del X_train\n",
    "    del X_valid\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/yf_lhm0x73s8q_ybcf1d10k00000gn/T/ipykernel_61405/38953050.py:4: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  columns = lf.columns\n"
     ]
    }
   ],
   "source": [
    "input_path = 'inputs/train.parquet/*/*.parquet'\n",
    "lf = pl.scan_parquet(input_path)\n",
    "#\n",
    "columns = lf.columns\n",
    "features_cols = [x for x in columns if 'feature' in x]\n",
    "responder_cols = [x for x in columns if 'responder' in x]\n",
    "target_col = 'responder_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2826.69 MB\n",
      "Memory usage after optimization is: 1431.47 MB\n",
      "Decreased by 49.4%\n",
      "(9500000, 79) (9500000,)\n",
      "CV Set:  (475000, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:43:48,516] A new study created in memory with name: no-name-74cfc51c-c8c6-439d-9516-fcba83cffb80\n",
      "[I 2024-12-24 22:45:16,243] Trial 0 finished with value: 0.8698565483093261 and parameters: {'booster': 'gbtree', 'lambda': 0.0008727825246365712, 'alpha': 1.0154084534195503e-08, 'subsample': 0.714031018688315, 'colsample_bytree': 0.3889380806471218, 'n_estimators': 50, 'max_depth': 9, 'min_child_weight': 10, 'eta': 0.00037443855342101534, 'gamma': 8.993097600031346e-07, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8698565483093261.\n",
      "[I 2024-12-24 22:45:32,279] Trial 1 finished with value: 0.863981568813324 and parameters: {'booster': 'gblinear', 'lambda': 0.0002894488237640431, 'alpha': 0.006133513618052148, 'subsample': 0.3186624182426906, 'colsample_bytree': 0.7827720275944052, 'n_estimators': 100}. Best is trial 1 with value: 0.863981568813324.\n",
      "[I 2024-12-24 22:50:12,315] Trial 2 finished with value: 0.8700449466705322 and parameters: {'booster': 'dart', 'lambda': 1.3276597648726582e-05, 'alpha': 1.9239143190552573e-06, 'subsample': 0.2700633832861768, 'colsample_bytree': 0.23144009602354132, 'n_estimators': 100, 'max_depth': 9, 'min_child_weight': 7, 'eta': 2.7540522152893432e-08, 'gamma': 7.29024338126854e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.0006603863666432416, 'skip_drop': 0.00020002317422825543}. Best is trial 1 with value: 0.863981568813324.\n",
      "[I 2024-12-24 22:54:49,016] Trial 3 finished with value: 0.9391827464103699 and parameters: {'booster': 'dart', 'lambda': 8.286295821367588e-06, 'alpha': 0.1371265107128123, 'subsample': 0.26187416736899166, 'colsample_bytree': 0.26757629449513376, 'n_estimators': 100, 'max_depth': 9, 'min_child_weight': 8, 'eta': 0.2772053272700512, 'gamma': 1.7399644644872463e-08, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.000390612062904841, 'skip_drop': 0.00014111083634370955}. Best is trial 1 with value: 0.863981568813324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  4\n",
      "Best trial:\n",
      "  Value: 0.863981568813324\n",
      "  Params: \n",
      "    booster: gblinear\n",
      "    lambda: 0.0002894488237640431\n",
      "    alpha: 0.006133513618052148\n",
      "    subsample: 0.3186624182426906\n",
      "    colsample_bytree: 0.7827720275944052\n",
      "    n_estimators: 100\n",
      "{'verbosity': 2, 'objective': 'reg:squarederror', 'tree_method': 'hist', 'booster': 'gblinear', 'lambda': 0.0002894488237640431, 'alpha': 0.006133513618052148, 'subsample': 0.3186624182426906, 'colsample_bytree': 0.7827720275944052, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/autocio/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [22:55:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"colsample_bytree\", \"subsample\", \"tree_method\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.93321\n",
      "[1]\tvalidation_0-rmse:0.93260\n",
      "[2]\tvalidation_0-rmse:0.93234\n",
      "[3]\tvalidation_0-rmse:0.93223\n",
      "[4]\tvalidation_0-rmse:0.93215\n",
      "[5]\tvalidation_0-rmse:0.93212\n",
      "[6]\tvalidation_0-rmse:0.93210\n",
      "[7]\tvalidation_0-rmse:0.93209\n",
      "[8]\tvalidation_0-rmse:0.93208\n",
      "[9]\tvalidation_0-rmse:0.93207\n",
      "[10]\tvalidation_0-rmse:0.93208\n",
      "[11]\tvalidation_0-rmse:0.93208\n",
      "[12]\tvalidation_0-rmse:0.93208\n",
      "[13]\tvalidation_0-rmse:0.93208\n",
      "[14]\tvalidation_0-rmse:0.93208\n",
      "[15]\tvalidation_0-rmse:0.93208\n",
      "[16]\tvalidation_0-rmse:0.93208\n",
      "[17]\tvalidation_0-rmse:0.93208\n",
      "[18]\tvalidation_0-rmse:0.93208\n",
      "[19]\tvalidation_0-rmse:0.93208\n",
      "[20]\tvalidation_0-rmse:0.93208\n",
      "[21]\tvalidation_0-rmse:0.93208\n",
      "[22]\tvalidation_0-rmse:0.93209\n",
      "[23]\tvalidation_0-rmse:0.93209\n",
      "[24]\tvalidation_0-rmse:0.93209\n",
      "[25]\tvalidation_0-rmse:0.93209\n",
      "[26]\tvalidation_0-rmse:0.93209\n",
      "[27]\tvalidation_0-rmse:0.93209\n",
      "[28]\tvalidation_0-rmse:0.93209\n",
      "[29]\tvalidation_0-rmse:0.93209\n",
      "[30]\tvalidation_0-rmse:0.93209\n",
      "[31]\tvalidation_0-rmse:0.93209\n",
      "[32]\tvalidation_0-rmse:0.93209\n",
      "[33]\tvalidation_0-rmse:0.93209\n",
      "[34]\tvalidation_0-rmse:0.93209\n",
      "[35]\tvalidation_0-rmse:0.93209\n",
      "[36]\tvalidation_0-rmse:0.93209\n",
      "[37]\tvalidation_0-rmse:0.93209\n",
      "[38]\tvalidation_0-rmse:0.93209\n",
      "[39]\tvalidation_0-rmse:0.93209\n",
      "[40]\tvalidation_0-rmse:0.93209\n",
      "[41]\tvalidation_0-rmse:0.93209\n",
      "[42]\tvalidation_0-rmse:0.93209\n",
      "[43]\tvalidation_0-rmse:0.93209\n",
      "[44]\tvalidation_0-rmse:0.93209\n",
      "[45]\tvalidation_0-rmse:0.93209\n",
      "[46]\tvalidation_0-rmse:0.93209\n",
      "[47]\tvalidation_0-rmse:0.93209\n",
      "[48]\tvalidation_0-rmse:0.93209\n",
      "[49]\tvalidation_0-rmse:0.93209\n",
      "[50]\tvalidation_0-rmse:0.93209\n",
      "[51]\tvalidation_0-rmse:0.93209\n",
      "[52]\tvalidation_0-rmse:0.93209\n",
      "[53]\tvalidation_0-rmse:0.93209\n",
      "[54]\tvalidation_0-rmse:0.93209\n",
      "[55]\tvalidation_0-rmse:0.93209\n",
      "[56]\tvalidation_0-rmse:0.93209\n",
      "[57]\tvalidation_0-rmse:0.93209\n",
      "[58]\tvalidation_0-rmse:0.93209\n",
      "[59]\tvalidation_0-rmse:0.93209\n",
      "[60]\tvalidation_0-rmse:0.93209\n",
      "[61]\tvalidation_0-rmse:0.93209\n",
      "[62]\tvalidation_0-rmse:0.93209\n",
      "[63]\tvalidation_0-rmse:0.93209\n",
      "[64]\tvalidation_0-rmse:0.93209\n",
      "[65]\tvalidation_0-rmse:0.93209\n",
      "[66]\tvalidation_0-rmse:0.93209\n",
      "[67]\tvalidation_0-rmse:0.93209\n",
      "[68]\tvalidation_0-rmse:0.93209\n",
      "[69]\tvalidation_0-rmse:0.93209\n",
      "[70]\tvalidation_0-rmse:0.93209\n",
      "[71]\tvalidation_0-rmse:0.93209\n",
      "[72]\tvalidation_0-rmse:0.93209\n",
      "[73]\tvalidation_0-rmse:0.93209\n",
      "[74]\tvalidation_0-rmse:0.93209\n",
      "[75]\tvalidation_0-rmse:0.93209\n",
      "[76]\tvalidation_0-rmse:0.93209\n",
      "[77]\tvalidation_0-rmse:0.93209\n",
      "[78]\tvalidation_0-rmse:0.93209\n",
      "[79]\tvalidation_0-rmse:0.93209\n",
      "[80]\tvalidation_0-rmse:0.93209\n",
      "[81]\tvalidation_0-rmse:0.93209\n",
      "[82]\tvalidation_0-rmse:0.93209\n",
      "[83]\tvalidation_0-rmse:0.93209\n",
      "[84]\tvalidation_0-rmse:0.93209\n",
      "[85]\tvalidation_0-rmse:0.93209\n",
      "[86]\tvalidation_0-rmse:0.93209\n",
      "[87]\tvalidation_0-rmse:0.93209\n",
      "[88]\tvalidation_0-rmse:0.93209\n",
      "[89]\tvalidation_0-rmse:0.93209\n",
      "[90]\tvalidation_0-rmse:0.93209\n",
      "[91]\tvalidation_0-rmse:0.93209\n",
      "[92]\tvalidation_0-rmse:0.93209\n",
      "[93]\tvalidation_0-rmse:0.93209\n",
      "[94]\tvalidation_0-rmse:0.93209\n",
      "[95]\tvalidation_0-rmse:0.93209\n",
      "[96]\tvalidation_0-rmse:0.93209\n",
      "[97]\tvalidation_0-rmse:0.93209\n",
      "[98]\tvalidation_0-rmse:0.93209\n",
      "[99]\tvalidation_0-rmse:0.93209\n",
      "Memory usage of dataframe is 2826.69 MB\n",
      "Memory usage after optimization is: 1431.47 MB\n",
      "Decreased by 49.4%\n",
      "(9500000, 79) (9500000,)\n",
      "CV Set:  (475000, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 23:06:25,212] A new study created in memory with name: no-name-d4a5b252-9a3f-4385-8ccc-65fa3b543e74\n",
      "[I 2024-12-24 23:10:08,269] Trial 0 finished with value: 0.8625175476074218 and parameters: {'booster': 'dart', 'lambda': 2.61108327657626e-08, 'alpha': 0.0013106545668249163, 'subsample': 0.708874382314868, 'colsample_bytree': 0.9322400609044399, 'n_estimators': 50, 'max_depth': 9, 'min_child_weight': 9, 'eta': 0.024014460733164534, 'gamma': 1.6915760467569681e-06, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.00010864184646829078, 'skip_drop': 1.1804067067678755e-06}. Best is trial 0 with value: 0.8625175476074218.\n",
      "[I 2024-12-24 23:10:49,123] Trial 1 finished with value: 0.8647468447685241 and parameters: {'booster': 'gblinear', 'lambda': 1.138380759227298e-08, 'alpha': 0.004856823470772233, 'subsample': 0.9041055227804782, 'colsample_bytree': 0.9775505131900493, 'n_estimators': 200}. Best is trial 0 with value: 0.8625175476074218.\n",
      "[I 2024-12-24 23:11:02,243] Trial 2 finished with value: 0.8705284237861634 and parameters: {'booster': 'gblinear', 'lambda': 4.430219701315783e-06, 'alpha': 0.7719382464659167, 'subsample': 0.751224081404992, 'colsample_bytree': 0.20300969662481602, 'n_estimators': 100}. Best is trial 0 with value: 0.8625175476074218.\n",
      "[I 2024-12-24 23:12:42,492] Trial 3 finished with value: 0.8705206036567688 and parameters: {'booster': 'dart', 'lambda': 4.112138254993005e-08, 'alpha': 0.00020860262327527244, 'subsample': 0.33533453008070657, 'colsample_bytree': 0.604542181126237, 'n_estimators': 50, 'max_depth': 5, 'min_child_weight': 9, 'eta': 2.1488293458209716e-05, 'gamma': 1.097119751010118e-06, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.6087014339585852e-07, 'skip_drop': 0.9970789227465349}. Best is trial 0 with value: 0.8625175476074218.\n",
      "[I 2024-12-24 23:15:49,349] Trial 4 finished with value: 0.8705168008804322 and parameters: {'booster': 'dart', 'lambda': 7.776998374856955e-05, 'alpha': 0.005034103506693033, 'subsample': 0.3630475601286576, 'colsample_bytree': 0.8946277860493566, 'n_estimators': 50, 'max_depth': 9, 'min_child_weight': 3, 'eta': 2.3097112794521515e-05, 'gamma': 2.5778310292004343e-07, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.034618559592731295, 'skip_drop': 1.455120189673078e-06}. Best is trial 0 with value: 0.8625175476074218.\n",
      "[I 2024-12-24 23:16:20,212] Trial 5 finished with value: 0.8644799947738647 and parameters: {'booster': 'gblinear', 'lambda': 0.0012353458408864307, 'alpha': 1.0558976949940169e-08, 'subsample': 0.6916228950976846, 'colsample_bytree': 0.2611919145234121, 'n_estimators': 200}. Best is trial 0 with value: 0.8625175476074218.\n",
      "[I 2024-12-24 23:33:12,148] Trial 6 finished with value: 0.8705277681350708 and parameters: {'booster': 'dart', 'lambda': 1.3487154256779977e-07, 'alpha': 0.6559704017430763, 'subsample': 0.2261226202090656, 'colsample_bytree': 0.6884828721863756, 'n_estimators': 200, 'max_depth': 7, 'min_child_weight': 2, 'eta': 3.9372165789400027e-07, 'gamma': 9.76373090163183e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 2.421270399031331e-06, 'skip_drop': 0.23447916917371356}. Best is trial 0 with value: 0.8625175476074218.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  7\n",
      "Best trial:\n",
      "  Value: 0.8625175476074218\n",
      "  Params: \n",
      "    booster: dart\n",
      "    lambda: 2.61108327657626e-08\n",
      "    alpha: 0.0013106545668249163\n",
      "    subsample: 0.708874382314868\n",
      "    colsample_bytree: 0.9322400609044399\n",
      "    n_estimators: 50\n",
      "    max_depth: 9\n",
      "    min_child_weight: 9\n",
      "    eta: 0.024014460733164534\n",
      "    gamma: 1.6915760467569681e-06\n",
      "    grow_policy: lossguide\n",
      "    sample_type: uniform\n",
      "    normalize_type: forest\n",
      "    rate_drop: 0.00010864184646829078\n",
      "    skip_drop: 1.1804067067678755e-06\n",
      "{'verbosity': 2, 'objective': 'reg:squarederror', 'tree_method': 'hist', 'booster': 'dart', 'lambda': 2.61108327657626e-08, 'alpha': 0.0013106545668249163, 'subsample': 0.708874382314868, 'colsample_bytree': 0.9322400609044399, 'n_estimators': 50, 'max_depth': 9, 'min_child_weight': 9, 'eta': 0.024014460733164534, 'gamma': 1.6915760467569681e-06, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.00010864184646829078, 'skip_drop': 1.1804067067678755e-06}\n",
      "[23:34:31] INFO: /Users/runner/work/xgboost/xgboost/src/gbm/gbtree.cc:918: drop 0 trees, weight = 1\n",
      "[0]\tvalidation_0-rmse:0.93113\n",
      "[01:08:17] INFO: /Users/runner/work/xgboost/xgboost/src/gbm/gbtree.cc:918: drop 0 trees, weight = 1\n",
      "[1]\tvalidation_0-rmse:0.93088\n",
      "[04:15:00] INFO: /Users/runner/work/xgboost/xgboost/src/gbm/gbtree.cc:918: drop 0 trees, weight = 1\n",
      "[2]\tvalidation_0-rmse:0.93063\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "num_models = 7\n",
    "for i in range(num_models):\n",
    "    model_name = f'xgb_{i}'\n",
    "    #\n",
    "    df = get_dataset(cols=features_cols+[target_col],head=int(1e7),fraction=0.95)\n",
    "    X = df[features_cols]\n",
    "    y = df[target_col]\n",
    "    print(X.shape, y.shape)\n",
    "    model = train_single_model(X, y, model_name, n_trials=30, cv_frac=0.01)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 5\n",
    "models = [xgb.XGBRegressor() for i in range(num_models)]\n",
    "for i, model in enumerate(models):\n",
    "    model.load_model(f'xgb_{i}.json')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch predicting\n",
    "# n = 47127338 \n",
    "# batch_size = 1e7\n",
    "n = 47127338\n",
    "batch_size = int(1e7)\n",
    "num_batch = n // batch_size\n",
    "lf = pl.scan_parquet(input_path)\n",
    "preds = []\n",
    "y_trues = []\n",
    "for i in range(num_batch+1):\n",
    "    print(f'Batch: {i}/{num_batch}')\n",
    "    rows = list(range(i*batch_size, min((i+1)*batch_size, n), 1))\n",
    "    df = lf.select(pl.all().gather(rows)).collect()\n",
    "    df = reduce_mem_usage(df.to_pandas())\n",
    "    _X_train = df[features_cols]\n",
    "    y_train = df[target_col]\n",
    "    X_train = [model.predict(_X_train) for model in models]\n",
    "    y_trues.append(y_train)\n",
    "    X_train = np.vstack(X_train).T\n",
    "    preds.append(X_train)\n",
    "    # booster = train_single_model(X_train, y_train, 'booster', n_trials=20)\n",
    "X_train_booster = np.row_stack(preds)\n",
    "pl.DataFrame(X_train_booster).write_parquet('X_train_booster.parquet')\n",
    "_y_trues = [x.to_numpy().reshape(-1,1) for x in y_trues]\n",
    "y_train_booster = np.row_stack(_y_trues)\n",
    "pl.DataFrame(y_train_booster).write_parquet('y_train_booster.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_booster = pl.read_parquet('X_train_booster.parquet').to_numpy()\n",
    "y_train_booster = pl.read_parquet('y_train_booster.parquet').to_numpy()\n",
    "print(X_train_booster.shape)\n",
    "print(y_train_booster.shape)\n",
    "# train booster\n",
    "booster = xgb.XGBRegressor( \n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    tree_method='hist',\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "booster.fit(X_train_booster, y_train_booster)\n",
    "booster.save_model('booster.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n",
    "# preds = []\n",
    "# n = 47127338\n",
    "# test_df = lf.select(pl.all().gather(list(range(n-10000,n,1)))).collect()\n",
    "X_test = test_df[features_cols].to_pandas()\n",
    "y_test = test_df[target_col].to_numpy()\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "preds = []\n",
    "for i, model in enumerate(models):\n",
    "    y_val_preds = model.predict(X_test)\n",
    "    preds.append(y_val_preds)\n",
    "    score = r2_score(y_true=y_test, y_pred=y_val_preds)\n",
    "    print(f'Model {i}: score: {score}')\n",
    "_preds = np.row_stack(preds).T\n",
    "_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster_preds = booster.predict(_preds)\n",
    "r2_score(y_true=y_test, y_pred=booster_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autocio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

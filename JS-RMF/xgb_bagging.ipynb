{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def objective(trial, x_train, x_test, y_train, y_test):\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        # \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 100, step=10),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    bst = xgb.XGBRegressor(**param)\n",
    "    fmse = make_scorer(mean_squared_error)\n",
    "    mean_cv = np.mean(cross_val_score(bst, x_train, y_train, cv=5, scoring=fmse))\n",
    "    return mean_cv\n",
    "\n",
    "def get_dataset(\n",
    "    cols: list[str],\n",
    "    input_paths: str = 'inputs/train.parquet/*/*.parquet',\n",
    "    fraction: float = 0.1,\n",
    "    head:int =None,\n",
    ") -> pd.DataFrame:\n",
    "    # data handling\n",
    "    lf = pl.scan_parquet(input_paths)\n",
    "    head = lf.select(pl.len()).collect()['len'][0] if head is None else head\n",
    "    df = lf.head(head).select(cols).collect()\n",
    "    df = df.sample(fraction=fraction).to_pandas()\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_single_model(X: pl.DataFrame, y: pl.DataFrame, model_name: str, n_trials: int = 100, timeout: int = 600):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    _objective = partial(objective, x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_test)\n",
    "    # hyper tune\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\n",
    "    study.optimize(_objective, n_trials=n_trials, timeout=timeout)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    params = {\n",
    "        \"verbosity\": 2,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        **trial.params,\n",
    "    }\n",
    "    print(params)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05)\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n",
    "    model.save_model(f'{model_name}.json')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'inputs/train.parquet/*/*.parquet'\n",
    "lf = pl.scan_parquet(input_path)\n",
    "#\n",
    "columns = lf.columns\n",
    "features_cols = [x for x in columns if 'feature' in x]\n",
    "responder_cols = [x for x in columns if 'responder' in x]\n",
    "target_col = 'responder_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "num_models = 5\n",
    "for i in range(num_models):\n",
    "    model_name = f'xgb_{i}'\n",
    "    #\n",
    "    df = get_dataset(cols=features_cols+[target_col],head=int(5*1e6),fraction=0.1)\n",
    "    X = df[features_cols]\n",
    "    y = df[target_col]\n",
    "    print(X.shape, y.shape)\n",
    "    model = train_single_model(X, y, model_name, n_trials=50)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset(cols=features_cols+[target_col], head=47127338, fraction=0.80)\n",
    "n_train = int(len(df) * 0.95)\n",
    "_X_train = df[features_cols][:n_train]\n",
    "y_train = df[target_col][:n_train]\n",
    "X_val = df[features_cols][n_train:]\n",
    "y_val = df[target_col][n_train:]\n",
    "X_train = [model.predict(_X_train) for model in models]\n",
    "X_train = np.vstack(X_train).T\n",
    "print(X_train.shape, y_train.shape)\n",
    "booster = train_single_model(X_train, y_train, 'booster', n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n",
    "preds = []\n",
    "for i, model in enumerate(models):\n",
    "    y_val_preds = model.predict(X_val)\n",
    "    preds.append(y_val_preds)\n",
    "    score = r2_score(y_true=y_val, y_pred=y_val_preds)\n",
    "    print(f'Model {i}: score: {score}')\n",
    "# booser score\n",
    "_preds = np.vstack(preds).T\n",
    "print(_preds.shape)\n",
    "booster_preds = booster.predict(_preds)\n",
    "r2_score(y_true=y_val, y_pred=booster_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autocio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
